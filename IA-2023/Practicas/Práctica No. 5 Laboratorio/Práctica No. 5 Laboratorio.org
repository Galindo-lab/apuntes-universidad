
#+TITLE:    Práctica No. 5 Laboratorio
#+SUBTITLE: 
#+AUTHOR:   Luis Eduardo Galindo Amaya
#+DATE:     sábado, 25 noviembre 2023

#+OPTIONS: toc:nil ^:nil title:nil num:2
#+LANGUAGE: es


#+latex_header: \usepackage{../../modern}
#+latex_header: \bibliography{./fuentes.bib}
#+latex_header: \raggedbottom

#+macro: code @@latex:\lstinputlisting{$1}@@
#+macro: cite @@latex:\cite{$1}@@
#+macro: autocite @@latex:\autocite{$1}@@

\modentitlepage{../../images/escudo-uabc-2022-1-tinta-pos.png}
\datasection{Individual}
\tableofcontents
\pagebreak



* Introducción
Durante esta practica utilizaremos sistemas de aprendizaje para
resolver problemas de clasificación y regreción con los problemas
propuestos en el taller anterior se utilizara la bibloteca sklearn y
los ejemplos dados en las clases anteriores. 

* Clasificación
** Problema
El problema seleccionado para utilizar clasificación es: Detección de
riesgo de ataque cardiaco. el dataset que se va a utilizar es '[[https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset?select=heart.css][Heart Attack
Analysis & Prediction Dataset]]' de kaggle.

** Desarollo
Para analizar el dataset primero fue necesario cargarlo en el proyecto
para esto se utilizo pandas, una biblioteca del python. Una vez que
tuve el dataset se identificaron las columnas que contenía el archivo:

#+ATTR_LATEX: :environment tabularx :width 0.8\textwidth
|   | Atributo   | Descripción                                |
| / | >          |                                            |
|---+------------+--------------------------------------------|
|   | Age        | Edad del paciente                          |
|   | Sex        | Género del paciente                        |
|   | exang      | Angina inducida por ejercicio              |
|   | ca         | Número de vasos principales                |
|   | cp         | Tipo de Dolor de Pecho                     |
|   | trtbps     | Presión arterial en reposo                 |
|   | chol       | Colesterol en mg/dl                        |
|   | fbs        | Azúcar en sangre en ayunas                 |
|   | rest_ecg   | Resultados electrocardiográficos en reposo |
|   | thalach    | Frecuencia cardíaca máxima alcanzada       |
|   | target     | Riesgo de ataque cardíaco                  |

La columna target nos permite identificar que pasientes estan en
riesgo de infarto de manera booleana (verdadero o falso), utilizando
la pagina de [[https://scikit-learn.org/][scikit-learn]] identifique que algoritmo me podria
clasificar de manera adecuada los datos que tenia y al ser datos de
tipo booleano la regrecion logística.

** Regresión logística
{{{autocite(wiki:Logistic_regression)}}} En estadísticas, el modelo
logístico (o modelo logit) es un modelo estadístico que modela la
probabilidad de que ocurra un evento al tener los logaritmos de las
probabilidades (log-odds) para el evento como una combinación lineal
de una o más variables independientes. La regrecion logistica utiliza
como base funcion logistica {{{autocite(wiki:Logistic_function)}}} la
cual toma un valor y retorna un valor entre 0 y 1:

\[
p(x) = \frac{1}{1 + e^{-( x - \mu )/s}}
\]

donde \(\mu\) es igual al punto medio de la curva y \(s\) es el valor
de escala, la formula entonce se puede reescribir de la manera:

\[
p(x) = \frac{1}{1 + e^{-( \beta_0 + \beta_1 x )}}
\]

donde \( \beta_0 = -\mu / s \) y a este punto se le conoce como la
intercepcion (la intercepcion vertical de \(y\)) y \(\beta_1 = 1/s\).

#+ATTR_HTML:
#+ATTR_LATEX: :width 6cm
#+CAPTION: Ejemplo de regresión logística
[[file:img/Exam_pass_logistic_curve.svg.png]]

** Resultados
El modelo da el resultado correcto y es capaz de clasificar
correctamente los valores que se le han enviado, a pesar de que se
movieron algunos parámetros pienso que el algoritmo seleccionado es
óptimo para esta aplicación. 

#+ATTR_HTML:
#+ATTR_LATEX: :width 10cm
#+CAPTION: Resultados de la clasificación
[[file:img/ddd.png]]

* Regresión
** Problema
La problematicas seleccionada para el problema de regresion fue la
prediccion de ingresos semanales de una empresa (Walmart), el dataset
que se utilizará es '[[https://www.kaggle.com/datasets/yasserh/walmart-dataset][Walmart dataset]]'.

** Desarollo
Primero se inspeccionaron los datos del dataset, y se pudo identificar
que no todos los datos se pueden usar para la regrecion: 

#+ATTR_LATEX: :environment tabularx :width 0.8\textwidth
|   | Atributo      | Descripción                                    |
|---+---------------+------------------------------------------------|
| / | >             |                                                |
|   | Store_Number  | Número de la tienda                            |
|   | Date          | Fecha                                          |
|   | Week_of_Sales | Semana de ventas                               |
|   | Weekly_Sales  | Ventas semanales para la tienda dada           |
|   | Holiday_Flag  | Indica si la semana es especial por festividad |
|   | Temperature   | Temperatura el día de la venta                 |
|   | Fuel_Price    | Costo del combustible en la región             |
|   | CPI           | Índice de precios al consumidor predominante   |
|   | Unemployment  | Tasa de desempleo predominante                 |

Los datos como el numero de la tienda y la fecha no se pueden usar
para la regreción, las fechas no son solo numeros y los
identificadores no estan ligadas a ningun otro valor que pueda ser
util.

** Elastic net regularization
{{{autocite(wiki:Elastic_net_regularization)}}} Para el problema de
regrecion se utilizara el algoritmo ElasticNet, Elastic Net es un
algoritmo de regresión utilizado en estadísticas y aprendizaje
automático. Este algoritmo combina las penalizaciones L1 y L2, que se
utilizan en las técnicas de regresión LASSO (Least Absolute Shrinkage
and Selection Operator) y Ridge, respectivamente. \\


{{{cite(wiki:LASSO_estadística)}}} Considere una clúster de N casos
(observaciones), cada una con p variable y una sola variable
independente. Sea \(y_i\) la variable independiente y
\(x_i := (x_1,x_2, ... x_p)^T \) el vector con variables para el caso
\(j\). Entonces, el objetivo del Lasso es resolver

\[
\min_{\beta_0, \beta}
\left\{
\frac{1}{N} \sum^N_{i=1}(y_i - \beta_0 -x_i^T \beta)^2
\right\}
\text{sujeto a}
\sum_{j=1}^p | \beta_j | \leq t.
\]

Aquí \(\beta_0\) es el coeficiente constante,
\( \beta:=(\beta_{1}, \beta_{2},..., \beta_{p}) \) es el vector de coeficientes
y \(t\) es un parámetro pre especificado que determine la cantidad de
regularizacion. Sea \(X\) la matriz de variables, de manera que
\(\displaystyle X_{ij}=(x_{i})_{j}\) y \(x_{i}^{T}\) es la 'i-esima
fila de \(X\), podemos escribir de forma más compacta el problema
como: 

\[
\min _{\beta _{0},\beta }\left\{{\frac {1}{N}}\left\|y-\beta
_{0}-X\beta \right\|_{2}^{2}\right\}{\text{ sujeto a }}\|\beta
\|_{1}\leq t.
\]


donde \( \|Z\|_{p}=\left(\sum _{i=1}^{N}|Z_{i}|^{p}\right)^{1/p}\)
es la p-norma en dimensiones finitas ( \(\displaystyle \ell ^{p}\) Espacios Lp).

Denotando la media escalar de los puntos \(x_{i}\) como \({\bar {x}}\)
y la media de las variables de salida como \(y_i\) como
\({\bar{y}}\), el estimado para \(\beta_0\) es \( {\hat {\beta }}_{0}={\bar
{y}}-{\bar {x}}^{T}\beta \), de modo que:

\[
y_{i}-{\hat {\beta }}_{0}-x_{i}^{T}\beta =y_{i}-({\bar {y}}
-{\bar {x}}^{T}\beta )-x_{i}^{T}\beta
=(y_{i}-{\bar {y}})-(x_{i}-{\bar {x}})^{T}\beta ,
\]

y así es estándar trabajar con variables centralizadas. Adicionalmente
las variables son estandarizadas \(\left(\sum _{i=1}^{N}x_{ij}^{2}=1\right)\)
para que la solución no sea afectada por la escala de las mediciones.

** Resultados
El modelo utilizado no es adecuado para la aplicación que se desea por
lo que los resultados obtenidos no pueden predecir los datos de manera
adecuada, se requiere de un mejor entendimiento de los datos para
poder seleccionar una tecnica más adecuada que pueda predecir los
datos obtenidos.

#+ATTR_HTML:
#+ATTR_LATEX: :width 5cm
#+CAPTION: Los datos predecidos no coinciden con las pruebas
[[file:img/output.png]]

* Conclusión
Durante esta practica pudimos utilizar herramientas de aprendizaje
para resolver problemas, las herramientas de machine learning permiten
resolver problemas que otras herramientas tendrían más dificultades
explicando. Herramientas como Sklearn permiten usar los algoritmos de
manera fácil sin tener que implementarlos cada vez además que
permiten probar diversas técnicas.


* Código
** Regresión
#+begin_src python -n
  from sklearn.model_selection import train_test_split     
  import matplotlib.pyplot as plt
  import numpy as np
  import pandas as pd
  from sklearn.pipeline import make_pipeline
  from sklearn.preprocessing import StandardScaler
  from sklearn.metrics import mean_squared_error
  from sklearn.linear_model import ElasticNet


  data = [
      'Store',
      'Holiday_Flag',
      'Temperature',
      'Fuel_Price',
      'CPI',
      'Unemployment'
  ]

  target = 'Weekly_Sales'

  dataset = pd.read_csv(
      "./Walmart.csv",
      sep=","
  )

  X_train, X_test, y_train, y_test = train_test_split(
       dataset[data], 
       dataset[target], 
       test_size=0.4
  )

  print(X_train, y_train)

  model = make_pipeline(
      StandardScaler(),
      ElasticNet(
          alpha=0.1,              # regularizacion del modelo
          l1_ratio=0.5            # tipo de de penalizacion
      )
  )

  model.fit(X_train, y_train)

  y_pred = model.predict(X_test)

  mse = mean_squared_error(y_test, y_pred)
  print(f"Mean Squared Error: {mse}")

  sim = model.predict(dataset[data])

  x = np.linspace(0,sim.size,sim.size)
  plt.style.use('_mpl-gallery')
  fig, ax = plt.subplots()
  ax.scatter(x, sim, s=1, c='blue')
  ax.scatter(x, dataset[target], s=1, c='red')
  plt.show()
#+end_src

** Clasificación
#+begin_src python -n
  from sklearn.model_selection import train_test_split     
  from sklearn.metrics import ConfusionMatrixDisplay
  from sklearn.linear_model import LogisticRegression
  import matplotlib.pyplot as plt
  import numpy as np
  import pandas as pd

  dataset = pd.read_csv(
      "./heart.csv", 
      sep=','
  )

  a = dataset[
      [
          "age","sex","cp","trtbps","chol","fbs",
          "restecg","thalachh","exng","oldpeak",
          "slp","caa","thall"
      ]
  ]

  X_train, X_test, y_train, y_test = train_test_split(
      a,
      dataset["output"],
      test_size=0.4
  )

  model = LogisticRegression(
      penalty="none",             # tipo de penalizacion 
      c=0.5                       # nivel de regularizacion 
  ).fit(X_train, y_train)

  sim = model.predict(X_test)
  print(y_test)
  print(sim)

  titles_options = [
      ("Confusion matrix, without normalization", None),
      ("Normalized confusion matrix", "true"),
  ]

  for title, normalize in titles_options:
      disp = ConfusionMatrixDisplay.from_estimator(
          model,
          X_test,
          y_test,
          display_labels=["positivo", "negativo"],
          cmap=plt.cm.Blues,
          normalize=normalize,
      )
      disp.ax_.set_title(title)

      print(title)
      print(disp.confusion_matrix)

   plt.show()
#+end_src

* Referencias
\printbibliography[heading=none]

